Title: Unified Vision-Language Representations via Cross-Modal Contrastive Alignment with Hard Negative Mining

Abstract

Multimodal learning, the integration of information from multiple sensory modalities such as vision and language, has gained tremendous momentum with the success of models like CLIP, ALIGN, and Flamingo. These models learn joint embedding spaces where images and their textual descriptions are mapped to nearby points, enabling zero-shot visual recognition, image-text retrieval, and visual question answering. However, the quality of learned representations is heavily dependent on the contrastive training objective, which can be undermined by uninformative negative samples that provide weak gradient signals. We propose Cross-Modal Hard Negative Contrastive Learning (CM-HNCo), a training framework that identifies and prioritizes challenging negative examples by mining hard negatives along both intra-modal and cross-modal dimensions. CM-HNCo introduces a dynamic negative sampling strategy that selects negatives based on their similarity to the anchor in one modality while being semantically dissimilar in the other, creating maximally informative contrastive pairs. On zero-shot ImageNet classification, CM-HNCo achieves 78.4% top-1 accuracy using a ViT-B/16 image encoder, surpassing CLIP's 76.2% with equivalent training data and compute.

1. Introduction

Human cognition is fundamentally multimodal: we perceive the world through the integrated processing of visual, auditory, and linguistic information. Building artificial systems that similarly integrate multiple modalities is a longstanding goal of artificial intelligence. Recent breakthroughs in vision-language models have been driven by large-scale contrastive pre-training, where models learn to associate images with their textual descriptions by contrasting matching (positive) image-text pairs against non-matching (negative) pairs.

The contrastive learning objective, typically formulated as InfoNCE loss, maximizes the cosine similarity of matching image-text pairs while minimizing the similarity of non-matching pairs within each training batch. The effectiveness of this objective depends critically on the quality of negative samples. In standard practice, negatives are simply other examples within the same mini-batch, which may be trivially distinguishable from the positive and thus uninformative. This is particularly problematic in vision-language learning, where subtle semantic distinctions (e.g., "a dog sitting on a bench" vs. "a dog standing near a bench") are crucial for learning fine-grained cross-modal alignment.

2. Methodology

CM-HNCo addresses negative sampling quality through three complementary mechanisms. First, we implement cross-modal hard negative mining by maintaining a memory bank of recent embeddings from both modalities and selecting negatives that have high similarity to the anchor in the embedding space of one modality but correspond to semantically different content. Second, we introduce synthetic hard negatives generated by applying controlled perturbations to positive captions using a lightweight language model, creating textually similar but semantically different descriptions (e.g., changing key attributes or spatial relationships). Third, we employ a curriculum-based difficulty schedule that gradually increases the proportion of hard negatives during training, preventing training instability that can arise from an exclusively hard negative regime in early training stages.

3. Results

On zero-shot ImageNet, CM-HNCo achieves 78.4% top-1 accuracy with ViT-B/16 and 81.2% with ViT-L/14, outperforming CLIP by 2.2 and 1.8 percentage points respectively. On the Flickr30K image-text retrieval benchmark, CM-HNCo achieves 97.2% Recall@1 for image retrieval and 88.6% for text retrieval. Analysis of the learned embedding space reveals tighter intra-class clustering and better separation of fine-grained categories compared to standard contrastive training.

4. Conclusion

CM-HNCo demonstrates that strategic hard negative mining significantly improves the quality of vision-language representations, enabling more discriminative cross-modal alignment without additional data or model capacity.
