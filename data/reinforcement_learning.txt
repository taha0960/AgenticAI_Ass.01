Title: Proximal Policy Optimization with Adaptive Entropy Regularization for Continuous Control Tasks

Abstract

Reinforcement learning (RL) provides a mathematical framework for sequential decision-making under uncertainty, where an agent learns to maximize cumulative reward through interaction with an environment. Despite significant progress in policy gradient methods, balancing exploration and exploitation remains a persistent challenge, particularly in high-dimensional continuous action spaces. In this work, we propose Adaptive Entropy-Regularized Proximal Policy Optimization (AE-PPO), an extension of the PPO algorithm that dynamically adjusts the entropy coefficient based on the agent's learning progress. We evaluate AE-PPO on a suite of MuJoCo continuous control benchmarks and demonstrate that it achieves superior sample efficiency and final performance compared to both standard PPO and SAC baselines, with an average improvement of 14.6% in cumulative return across six locomotion tasks.

1. Introduction

The reinforcement learning problem is formalized as a Markov Decision Process (MDP), defined by the tuple (S, A, P, R, gamma), where S is the state space, A is the action space, P represents transition dynamics, R is the reward function, and gamma is the discount factor. The objective is to learn a policy pi(a|s) that maximizes the expected discounted return E[sum_{t=0}^{inf} gamma^t r_t]. Policy gradient methods estimate the gradient of this objective directly and update the policy parameters in the direction of improvement.

Proximal Policy Optimization (PPO) has emerged as a widely adopted policy gradient algorithm due to its simplicity and robustness. PPO constrains policy updates by clipping the probability ratio between the new and old policies, preventing destabilizing large updates. However, PPO employs a fixed entropy bonus coefficient to encourage exploration, which may be suboptimal: too much entropy early in training impedes convergence, while too little entropy later may cause premature convergence to suboptimal policies.

2. Methodology

AE-PPO addresses this limitation by introducing an adaptive entropy regularization scheme. We maintain a target entropy value H_target, set proportional to the action space dimensionality, and adjust the entropy coefficient alpha using dual gradient descent. Specifically, we minimize the loss L(alpha) = -alpha * (H(pi) - H_target) with respect to alpha at each policy update step. This formulation automatically increases the entropy coefficient when the policy becomes overly deterministic and decreases it when the policy is sufficiently exploratory, providing a self-tuning mechanism that adapts to the learning dynamics.

3. Experimental Results

We evaluate AE-PPO on six MuJoCo locomotion tasks: HalfCheetah, Walker2d, Hopper, Ant, Humanoid, and Swimmer. Each experiment is conducted over five random seeds with 3 million environment steps. AE-PPO achieves the highest mean return on five out of six tasks, with particularly pronounced improvements on the Humanoid task (22.3% improvement over PPO), where the high-dimensional action space makes fixed entropy scheduling especially problematic. Learning curves indicate that AE-PPO converges faster in the early training phase while maintaining stable improvement throughout training.

4. Conclusion

AE-PPO demonstrates that adaptive entropy regularization is a simple yet effective modification that meaningfully improves PPO's performance in continuous control settings. The approach introduces minimal computational overhead and requires no additional hyperparameter tuning beyond the target entropy, making it a practical enhancement for practitioners.
