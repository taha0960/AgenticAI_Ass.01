Title: Revisiting Scaled Dot-Product Attention: Architectural Innovations in Transformer Networks for Sequence Modeling

Abstract

The Transformer architecture has fundamentally reshaped the landscape of deep learning since its introduction by Vaswani et al. (2017). This paper provides a comprehensive analysis of the self-attention mechanism and its variants, examining how multi-head attention enables parallel computation of contextual representations across sequence elements. We propose a modified attention formulation that incorporates relative positional encodings with learnable gating parameters, achieving improved performance on long-range dependency tasks. Our experiments on standard benchmarks demonstrate a 3.2% improvement in BLEU score on WMT'14 English-to-German translation and a 1.8% reduction in perplexity on the WikiText-103 language modeling benchmark.

1. Introduction

Sequential data processing has long been dominated by recurrent architectures such as LSTMs and GRUs, which process tokens in a strictly sequential manner. The Transformer architecture departed from this paradigm by replacing recurrence with self-attention, enabling each position in a sequence to attend directly to all other positions. This architectural choice yields two critical advantages: first, it reduces the path length between any two positions from O(n) to O(1), facilitating the learning of long-range dependencies; second, it permits full parallelization of computation across sequence positions during training.

The core computation in a Transformer layer is the scaled dot-product attention, defined as Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V, where Q, K, and V represent query, key, and value matrices respectively, and d_k is the dimensionality of the key vectors. Multi-head attention extends this by projecting the input into multiple subspaces and computing attention independently in each, allowing the model to capture diverse relational patterns simultaneously.

2. Methodology

We introduce Gated Relative Attention (GRA), which augments standard multi-head attention with two modifications. First, we replace absolute sinusoidal positional encodings with relative positional biases computed as a function of the distance between token positions. Second, we introduce a learnable gating mechanism that modulates the contribution of attention scores based on layer depth, enabling the network to adaptively balance local and global information flow. The gating parameter is implemented as a sigmoid-activated scalar per attention head, initialized to 0.5 to preserve backward compatibility with standard attention at initialization.

3. Results and Discussion

We evaluate GRA on machine translation, language modeling, and abstractive summarization tasks. On WMT'14 En-De translation, our model achieves a BLEU score of 29.7, compared to 28.8 for the baseline Transformer-Base configuration. On WikiText-103, we observe a test perplexity of 17.4, improving upon the baseline of 17.7. Ablation studies confirm that both the relative positional encoding and the gating mechanism contribute independently to the observed improvements, with their combination yielding synergistic gains. Analysis of learned gating values reveals that lower layers tend to favor local attention patterns while upper layers maintain broader attention distributions, consistent with linguistic intuitions about hierarchical language processing.

4. Conclusion

Our work demonstrates that targeted architectural modifications to the attention mechanism can yield meaningful improvements in Transformer performance without significant computational overhead. The Gated Relative Attention mechanism offers a principled approach to balancing positional sensitivity and contextual breadth across network depth.
