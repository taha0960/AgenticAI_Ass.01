Title: Cross-Domain Transfer Learning with Progressive Feature Alignment for Low-Resource Visual Recognition

Abstract

Transfer learning has become the de facto approach for tackling visual recognition tasks where labeled data is scarce, leveraging representations learned from large-scale source domains such as ImageNet. However, the effectiveness of transfer learning diminishes when there is significant domain shift between the source and target distributions, a challenge commonly encountered in specialized applications like satellite imagery analysis, medical pathology, and industrial defect detection. We propose Progressive Feature Alignment (PFA), a domain adaptation framework that gradually aligns source and target feature distributions across multiple stages of increasing granularity. PFA operates by first aligning global domain statistics through moment matching, then progressively refining alignment at the level of semantic clusters and finally individual class-conditional distributions. Evaluated on four standard domain adaptation benchmarks (Office-31, Office-Home, VisDA-2017, and DomainNet), PFA achieves an average classification accuracy of 89.7%, surpassing the previous state-of-the-art by 2.3 percentage points while requiring 35% fewer training iterations to converge.

1. Introduction

The success of deep learning in computer vision is predicated on the availability of large annotated datasets. In practice, obtaining sufficient labeled examples for every target task is often infeasible due to the cost of expert annotation, rarity of certain categories, or domain-specific constraints. Transfer learning addresses this by initializing models with weights pre-trained on a data-rich source domain and adapting them to the target domain through fine-tuning. When the source and target domains share similar visual characteristics, this approach is remarkably effective. However, substantial domain shift, arising from differences in imaging conditions, visual styles, or object appearance, can cause negative transfer where pre-trained features are poorly suited to the target distribution.

Domain adaptation methods seek to mitigate domain shift by explicitly aligning the feature representations of source and target domains. Adversarial domain adaptation methods train a domain discriminator alongside the task classifier, encouraging domain-invariant features. Discrepancy-based methods minimize statistical distance measures such as Maximum Mean Discrepancy (MMD) between domain feature distributions. Despite their effectiveness, these methods typically apply alignment uniformly across the feature space, ignoring the fact that different aspects of the domain shift may be best addressed at different levels of representational granularity.

2. Methodology

PFA addresses this limitation through a three-stage progressive alignment curriculum. In Stage 1 (epochs 1-10), we align global domain statistics by matching the first and second moments (mean and covariance) of source and target feature distributions at the penultimate layer. In Stage 2 (epochs 11-25), we perform cluster-level alignment by applying k-means clustering to target features and aligning each cluster to its nearest source-domain semantic prototype. In Stage 3 (epochs 26-50), we refine alignment at the class-conditional level using pseudo-labels generated by the model's current predictions on target data, with a confidence-based filtering mechanism to mitigate label noise.

3. Results

On Office-Home (65 categories, 4 domains), PFA achieves an average accuracy of 74.8%, a 2.1-point improvement over CDAN+E. On the large-scale DomainNet benchmark (345 categories, 6 domains), PFA obtains 45.6% average accuracy, exceeding MCC by 1.8 points. Convergence analysis shows that the progressive curriculum enables faster training, as early-stage global alignment provides a strong initialization for the more fine-grained later stages.

4. Conclusion

Progressive Feature Alignment demonstrates that domain adaptation benefits from structured, multi-granularity alignment strategies that match the hierarchical nature of visual feature representations, offering both improved accuracy and faster convergence.
